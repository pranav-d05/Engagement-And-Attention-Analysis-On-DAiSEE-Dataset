{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (update this part with your dataset)\n",
    "X_train = np.load(r\"P:\\Project\\train\\X_train.npy\")\n",
    "y_train = np.load(r\"P:\\Project\\train\\y_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution: Counter({2: 2617, 3: 2493, 1: 213, 0: 34})\n"
     ]
    }
   ],
   "source": [
    "print(\"New class distribution:\", Counter(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution: Counter({1: 2830, 0: 2651, 2: 2617, 3: 2493})\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Count current class distribution\n",
    "class_counts = Counter(y_train)\n",
    "\n",
    "# Define target count (matching Class 2)\n",
    "target_count = class_counts[2]  \n",
    "\n",
    "# Get indices of class 0 and 1\n",
    "class_0_indices = np.where(y_train == 0)[0]\n",
    "class_1_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "# Randomly duplicate samples from class 0 and 1 until they reach the target count\n",
    "oversampled_0 = np.random.choice(class_0_indices, size=target_count, replace=True)\n",
    "oversampled_1 = np.random.choice(class_1_indices, size=target_count, replace=True)\n",
    "\n",
    "# Combine original data with oversampled data\n",
    "X_train_oversampled = np.concatenate([X_train, X_train[oversampled_0], X_train[oversampled_1]])\n",
    "y_train_oversampled = np.concatenate([y_train, y_train[oversampled_0], y_train[oversampled_1]])\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffle_indices = np.random.permutation(len(y_train_oversampled))\n",
    "X_train_oversampled = X_train_oversampled[shuffle_indices]\n",
    "y_train_oversampled = y_train_oversampled[shuffle_indices]\n",
    "\n",
    "# Print new class distribution\n",
    "print(\"New class distribution:\", Counter(y_train_oversampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Class Weights: {0: 39.38970588235294, 1: 6.287558685446009, 2: 0.5117500955292319, 3: 0.537204171680706}\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train.flatten()\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(\"Computed Class Weights:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from new_hyperparam_tuning\\lstm_tuning\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Train-validation split (80-20)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train_oversampled, y_train_oversampled, test_size=0.2, random_state=42, stratify=y_train_oversampled)\n",
    "\n",
    "# Define the model-building function\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(\n",
    "        units=hp.Int(\"num_units\", min_value=128, max_value=512, step=128),\n",
    "        return_sequences=False,\n",
    "        input_shape=(50, 2048)\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Dropout(hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    model.add(tf.keras.layers.Dense(4, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = hp.Choice(\"optimizer\", [\"adam\", \"rmsprop\"])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Define the tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=10,  # Number of different models to try\n",
    "    executions_per_trial=1,  # Number of times to train each model\n",
    "    directory=\"new_hyperparam_tuning\",\n",
    "    project_name=\"lstm_tuning\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 12m 11s]\n",
      "val_accuracy: 0.7621519565582275\n",
      "\n",
      "Best val_accuracy So Far: 0.7649834752082825\n",
      "Total elapsed time: 01h 09m 23s\n",
      "Best Hyperparameters: Units=384, Dropout=0.1, Optimizer=adam\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(x_train, y_train, epochs=10, validation_data=(x_val, y_val), verbose=1)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: Units={best_hps.get('num_units')}, Dropout=0.15, Optimizer={best_hps.get('optimizer')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 192ms/step - accuracy: 0.4459 - loss: 1.2363 - val_accuracy: 0.6092 - val_loss: 0.8372\n",
      "Epoch 2/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 187ms/step - accuracy: 0.6056 - loss: 0.8619 - val_accuracy: 0.6734 - val_loss: 0.7347\n",
      "Epoch 3/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 187ms/step - accuracy: 0.6701 - loss: 0.7377 - val_accuracy: 0.7013 - val_loss: 0.6914\n",
      "Epoch 4/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 185ms/step - accuracy: 0.6917 - loss: 0.6832 - val_accuracy: 0.6357 - val_loss: 0.7885\n",
      "Epoch 5/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 185ms/step - accuracy: 0.6896 - loss: 0.6819 - val_accuracy: 0.6626 - val_loss: 0.6834\n",
      "Epoch 6/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 185ms/step - accuracy: 0.7276 - loss: 0.6040 - val_accuracy: 0.7220 - val_loss: 0.5964\n",
      "Epoch 7/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 187ms/step - accuracy: 0.7436 - loss: 0.5777 - val_accuracy: 0.7272 - val_loss: 0.5739\n",
      "Epoch 8/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7473 - loss: 0.5587 - val_accuracy: 0.7343 - val_loss: 0.5354\n",
      "Epoch 9/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7549 - loss: 0.5263 - val_accuracy: 0.7022 - val_loss: 0.6197\n",
      "Epoch 10/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7527 - loss: 0.5464 - val_accuracy: 0.7555 - val_loss: 0.5216\n",
      "Epoch 11/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7879 - loss: 0.4630 - val_accuracy: 0.7702 - val_loss: 0.5015\n",
      "Epoch 12/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7783 - loss: 0.4808 - val_accuracy: 0.7607 - val_loss: 0.4915\n",
      "Epoch 13/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7818 - loss: 0.4671 - val_accuracy: 0.7371 - val_loss: 0.5875\n",
      "Epoch 14/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.8001 - loss: 0.4366 - val_accuracy: 0.7711 - val_loss: 0.4887\n",
      "Epoch 15/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 187ms/step - accuracy: 0.7960 - loss: 0.4395 - val_accuracy: 0.7768 - val_loss: 0.4756\n",
      "Epoch 16/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.8072 - loss: 0.4216 - val_accuracy: 0.7555 - val_loss: 0.5093\n",
      "Epoch 17/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.7983 - loss: 0.4305 - val_accuracy: 0.7636 - val_loss: 0.5053\n",
      "Epoch 18/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.8043 - loss: 0.4246 - val_accuracy: 0.7872 - val_loss: 0.4335\n",
      "Epoch 19/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.8238 - loss: 0.3811 - val_accuracy: 0.7886 - val_loss: 0.4475\n",
      "Epoch 20/20\n",
      "\u001b[1m265/265\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 186ms/step - accuracy: 0.8102 - loss: 0.3942 - val_accuracy: 0.7824 - val_loss: 0.4436\n"
     ]
    }
   ],
   "source": [
    "# Build and train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       530\n",
      "           1       0.84      0.99      0.91       566\n",
      "           2       0.59      0.65      0.62       524\n",
      "           3       0.66      0.45      0.54       499\n",
      "\n",
      "    accuracy                           0.78      2119\n",
      "   macro avg       0.77      0.77      0.77      2119\n",
      "weighted avg       0.78      0.78      0.77      2119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get model predictions\n",
    "y_pred = best_model.predict(x_val)\n",
    "y_pred_classes = y_pred.argmax(axis=1)  # Convert probabilities to class labels\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_val, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the best model\n",
    "best_model.save(\"best_lstm_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
